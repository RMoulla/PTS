{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/PTS/blob/main/LSTM_Classification_Correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYNAnKThFDX7"
      },
      "source": [
        "# Entraînement d'un modèle LSTM pour la Classification de texte\n",
        "\n",
        "Dans ce tutoriel, nous allons aborder les étapes essentielles pour construire et entraîner un modèle de réseau de neurones récurrents, spécifiquement une architecture LSTM (Long Short-Term Memory), pour la classification de texte en utilisant le jeu de données AG_NEWS.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "Le jeu de données AG_NEWS est un ensemble de données de classification des nouvelles qui comprend des titres d'articles de nouvelles et des descriptions provenant de plus de 2 000 sources d'informations. Il est organisé en quatre catégories principales : Monde, Sports, Business et Science/Technologie, ce qui en fait un choix excellent pour les tâches de classification de texte dans le domaine de la reconnaissance automatique des sujets de nouvelles. Chaque enregistrement est étiqueté avec son appartenance à l'une de ces catégories, offrant ainsi un cadre clair pour les modèles supervisés d'apprentissage automatique.\n",
        "\n",
        "\n",
        "\n",
        "### Préparation de l'environnement  \n",
        "Nous commencerons par configurer notre environnement de développement en important les bibliothèques nécessaires fournies par PyTorch, y compris les modules pour les jeux de données, la modélisation, et l'optimisation.\n",
        "\n",
        "### Chargement des données\n",
        "Nous chargerons le jeu de données AG_NEWS en utilisant les utilitaires de torchtext, qui simplifient le processus de téléchargement et de prétraitement des données textuelles.\n",
        "\n",
        "### Prétraitement du texte\n",
        "Nous aborderons ensuite les étapes de tokenisation et de construction de vocabulaire, deux étapes préliminaires cruciales pour transformer le texte brut en une forme numérique que notre modèle pourra interpréter.\n",
        "\n",
        "### Construction du modèle LSTM\n",
        "Nous définirons la classe LSTMClassifier, qui établira l'architecture de notre modèle LSTM en utilisant les couches et fonctions fournies par PyTorch.\n",
        "\n",
        "### Entraînement du modèle  \n",
        "Nous procéderons à l'entraînement du modèle en utilisant la rétropropagation à travers le temps (Backpropagation Through Time, BPTT), tout en gérant les aspects tels que la fonction de perte et l'optimisation.\n",
        "\n",
        "### Évaluation du modèle  \n",
        "Nous évaluerons les performances de notre modèle en calculant la précision sur un ensemble de test, ce qui nous donnera une indication de la façon dont notre modèle est susceptible de se comporter en production.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFANCW1PeCOS",
        "outputId": "89188650-23d8-42c9-819c-fc6d40edf3c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0) (4.12.2)\n",
            "Requirement already satisfied: torchtext==0.12.0 in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (2.31.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (1.25.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0->torchtext==0.12.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (2024.6.2)\n",
            "Requirement already satisfied: torchdata==0.3.0 in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.3.0) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.3.0) (2.31.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.3.0) (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0->torchdata==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.3.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.3.0) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.3.0) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install portalocker>=2.0.0\n",
        "!pip install torch==1.11.0\n",
        "!pip install torchtext==0.12.0\n",
        "!pip install torchdata==0.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9or2WVliLYH_",
        "outputId": "1d280399-b713-4e4f-fa2a-3049e267e6b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n"
          ]
        }
      ],
      "source": [
        "from torchtext.datasets import AG_NEWS\n",
        "\n",
        "# Crée un itérateur\n",
        "train_iter = iter(AG_NEWS(split='train'))\n",
        "\n",
        "# Récupère le premier exemple\n",
        "first_example = next(train_iter)\n",
        "\n",
        "print(first_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeiud_eMBoYG",
        "outputId": "4a8cb6b5-365f-46e7-c07a-740b2e2aab1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.5888835029420132, Test Loss: 0.042107843788046585, Test Accuracy: 0.8885526315789474\n",
            "Epoch 2, Train Loss: 0.23292351463234517, Test Loss: 0.036231964400718235, Test Accuracy: 0.9073684210526316\n",
            "Epoch 3, Train Loss: 0.16023908399071854, Test Loss: 0.037369000022245975, Test Accuracy: 0.9132894736842105\n",
            "Epoch 4, Train Loss: 0.11205181304087648, Test Loss: 0.040376639581552715, Test Accuracy: 0.9144736842105263\n",
            "Epoch 5, Train Loss: 0.07772342970091098, Test Loss: 0.04390877313484644, Test Accuracy: 0.9101315789473684\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Détecte si un GPU est disponible et le sélectionne pour les calculs, sinon utilise le CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Charge les ensembles de données d'entraînement et de test AG News.\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "\n",
        "# Initialise le tokenizer pour la tokenisation en anglais basique.\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Définit une fonction génératrice qui itère sur l'ensemble de données et tokenize le texte.\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# Construit un vocabulaire à partir de l'itérateur de tokens avec un token inconnu spécial \"<unk>\".\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Définit une classe de modèle pour le classificateur LSTM en utilisant le module nn de PyTorch.\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Couche d'embedding\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)  # Couche LSTM\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)  # Couche de sortie\n",
        "\n",
        "    def forward(self, text):\n",
        "        # Définit le flux de données à travers le réseau.\n",
        "        embedded = self.embedding(text)  # Passe le texte tokenisé à travers la couche d'embedding\n",
        "        lstm_out, _ = self.lstm(embedded)  # Passe les embeddings à travers la couche LSTM\n",
        "        final_out = lstm_out[:, -1, :]  # Utilise la dernière sortie cachée de la séquence\n",
        "        return self.fc(final_out)  # Passe la dernière sortie à travers la couche fully-connected\n",
        "\n",
        "# Initialise les paramètres du modèle.\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 64\n",
        "hidden_dim = 128\n",
        "num_classes = 4  # Les catégories sont Monde, Sports, Business, Science/Technologie\n",
        "\n",
        "# Crée une instance du modèle LSTMClassifier et la transfère sur le périphérique sélectionné (GPU ou CPU).\n",
        "model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_classes).to(device)\n",
        "\n",
        "# Définit la fonction de perte CrossEntropy pour la classification et l'optimiseur Adam.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Prépare les données pour l'entraînement avec une fonction de collation personnalisée.\n",
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    Prépare un lot pour l'entraînement ou l'évaluation.\n",
        "\n",
        "    Cette fonction est utilisée comme fonction de collation pour le DataLoader. Elle traite chaque lot\n",
        "    en séparant les étiquettes des textes, tokenisant les textes, convertissant les tokens en indices\n",
        "    via le vocabulaire, puis padde les séquences pour qu'elles aient toutes la même longueur.\n",
        "\n",
        "    Args:\n",
        "    batch (list): Un lot d'échantillons provenant du DataLoader, où chaque échantillon est un tuple\n",
        "                  contenant l'étiquette et le texte brut.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[Tensor, Tensor]: Le premier tensor contient les étiquettes du lot, et le second tensor\n",
        "                           contient les textes tokenisés et paddés du lot, tous deux prêts à être\n",
        "                           passés à travers le modèle.\n",
        "    \"\"\"\n",
        "    label_list, text_list = [], []  # Initialisation des listes pour stocker séparément les étiquettes et les textes traités.\n",
        "\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(_label - 1)  # Ajuste les étiquettes pour la perte de CrossEntropy et les ajoute à la liste des étiquettes.\n",
        "        processed_text = torch.tensor(vocab(tokenizer(_text)), dtype=torch.int64)  # Tokenise le texte, convertit les tokens en indices, et crée un tensor.\n",
        "        text_list.append(processed_text)  # Ajoute le texte traité à la liste des textes.\n",
        "\n",
        "    # Convertit les listes d'étiquettes et de textes en tensors, puis les transfère sur le dispositif approprié (GPU ou CPU).\n",
        "    # Padded les séquences de textes pour qu'elles aient toutes la même longueur.\n",
        "    return torch.tensor(label_list, dtype=torch.int64).to(device), nn.utils.rnn.pad_sequence(text_list, batch_first=True).to(device)\n",
        "\n",
        "\n",
        "# Crée des chargeurs de données pour l'entraînement et les tests.\n",
        "train_loader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# Définit une fonction pour entraîner le modèle.\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, device):\n",
        "    \"\"\"\n",
        "    Entraîner le modèle LSTM avec les données fournies.\n",
        "\n",
        "    Args:\n",
        "    model (torch.nn.Module): Le modèle LSTM à entraîner.\n",
        "    train_loader (DataLoader): Chargeur de données pour l'ensemble d'entraînement.\n",
        "    test_loader (DataLoader): Chargeur de données pour l'ensemble de test.\n",
        "    criterion (loss_function): Fonction de perte à utiliser pour l'entraînement.\n",
        "    optimizer (torch.optim): Optimiseur pour mettre à jour les poids du modèle.\n",
        "    num_epochs (int): Nombre d'époques d'entraînement.\n",
        "    device (torch.device): Dispositif sur lequel le modèle est exécuté (CPU ou GPU).\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    for epoch in range(num_epochs):  # Boucle sur le nombre d'époques spécifié.\n",
        "        model.train()  # Met le modèle en mode entraînement.\n",
        "        total_loss = 0  # Accumulateur pour la perte totale sur l'ensemble d'entraînement.\n",
        "        num_batches = 0  # Compteur pour le nombre de lots traités.\n",
        "\n",
        "        for labels, text in train_loader:\n",
        "            num_batches += 1  # Incrémente le compteur de lots.\n",
        "            labels, text = labels.to(device), text.to(device)  # Transfère les données sur le dispositif approprié.\n",
        "            optimizer.zero_grad()  # Réinitialise les gradients de l'optimiseur pour le nouveau lot.\n",
        "            output = model(text)  # Calcule la sortie du modèle pour le lot actuel.\n",
        "            loss = criterion(output, labels)  # Calcule la perte entre les prédictions et les étiquettes réelles.\n",
        "            loss.backward()  # Calcule les gradients de la perte par rapport aux poids du modèle.\n",
        "            optimizer.step()  # Met à jour les poids du modèle en fonction des gradients calculés.\n",
        "            total_loss += loss.item()  # Ajoute la perte du lot à la perte totale pour cette époque.\n",
        "\n",
        "        # Calcule la perte moyenne d'entraînement sur tous les lots pour l'époque.\n",
        "        train_loss = total_loss / num_batches\n",
        "        # Évalue le modèle sur l'ensemble de test pour obtenir la perte et la précision du test.\n",
        "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "        # Affiche les statistiques de performance après chaque époque.\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Évaluer le modèle sur l'ensemble de test.\n",
        "\n",
        "    Args:\n",
        "    model (torch.nn.Module): Le modèle LSTM à évaluer.\n",
        "    test_loader (DataLoader): Chargeur de données pour l'ensemble de test.\n",
        "    criterion (loss_function): Fonction de perte utilisée pour l'évaluation.\n",
        "    device (torch.device): Dispositif sur lequel le modèle est exécuté (CPU ou GPU).\n",
        "\n",
        "    Returns:\n",
        "    float: La perte moyenne sur l'ensemble de test.\n",
        "    float: La précision du modèle sur l'ensemble de test.\n",
        "    \"\"\"\n",
        "    model.eval()  # Met le modèle en mode évaluation pour désactiver le dropout ou la normalisation par lots, si présent.\n",
        "    total_loss = 0  # Accumulateur pour la perte totale sur l'ensemble de test.\n",
        "    correct_preds = 0  # Compteur pour le nombre total de prédictions correctes.\n",
        "    total_samples = 0  # Compteur pour le nombre total d'échantillons dans l'ensemble de test.\n",
        "\n",
        "    with torch.no_grad():  # Désactive la génération de gradients pour les opérations suivantes, ce qui économise de la mémoire et des calculs.\n",
        "        for labels, text in test_loader:\n",
        "            labels, text = labels.to(device), text.to(device)  # Transfère les données sur le dispositif approprié.\n",
        "            output = model(text)  # Obtient les prédictions du modèle pour le lot actuel.\n",
        "            loss = criterion(output, labels)  # Calcule la perte entre les prédictions et les étiquettes réelles.\n",
        "            total_loss += loss.item()  # Ajoute la perte du lot à la perte totale.\n",
        "            predictions = output.argmax(1)  # Obtient les indices des prédictions maximales (classes prédites).\n",
        "            correct_preds += (predictions == labels).sum().item()  # Incrémente le nombre de prédictions correctes.\n",
        "            total_samples += labels.size(0)  # Ajoute le nombre d'échantillons dans le lot au nombre total d'échantillons.\n",
        "\n",
        "    # Calcule la perte moyenne et la précision sur l'ensemble de test.\n",
        "    return total_loss / total_samples, correct_preds / total_samples\n",
        "\n",
        "\n",
        "# Entraîne et évalue le modèle\n",
        "num_epochs = 5  # Définit le nombre d'itérations pour l'entraînement.\n",
        "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNQ6gtH1ujSzZRrLIs0tmvc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}