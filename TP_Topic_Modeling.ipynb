{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO204Ej77FxZf0wd5noubHc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/PTS/blob/main/TP_Topic_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling sur des avis de consommateurs\n",
        "\n",
        "Dans ce tutoriel, nous allons utiliser une technique de topic modeling pour extraire des thématiques significatives à partir de données textuelles. Le jeu de données choisi est composé d'avis de clients sur des produits électroniques vendus sur Amazon. Ce dataset comprend des informations telles que le texte de l'avis, la note globale attribuée au produit, un résumé de l'avis, ainsi que des informations temporelles liées à chaque avis.\n",
        "\n",
        "La technique utilisée ici est Latent Dirichlet Allocation (LDA), un modèle statistique adapté pour l'identification de thématiques latentes dans de vastes ensembles de données textuelles.\n",
        "\n",
        "LDA est un modèle génératif probabiliste qui permet de découvrir des groupes de mots (appelés \"topics\" ou thématiques) dans un corpus de documents. Il est basé sur deux hypothèses principales :\n",
        "\n",
        "* Chaque document est un mélange de sujets : LDA suppose que chaque document dans notre ensemble de données peut être décrit par un certain nombre de sujets différents. Par exemple, un avis sur un smartphone pourrait inclure des sujets tels que la performance de la batterie, la qualité de l'appareil photo, ou l'expérience utilisateur.\n",
        "\n",
        "* Chaque sujet est un mélange de mots : Chaque sujet identifié par LDA est caractérisé par un ensemble de mots. Par exemple, le sujet \"performance de la batterie\" pourrait inclure des mots comme \"longue durée\", \"recharge\", \"autonomie\", etc.\n"
      ],
      "metadata": {
        "id": "-_ffM4aqMM6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyldavis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--CAARJ_1n-r",
        "outputId": "c2eafca7-ac8c-4e41-a46f-bec4a1a13b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyldavis in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyldavis) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyldavis) (1.11.4)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyldavis) (2.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyldavis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyldavis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyldavis) (2.10.1)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.10/dist-packages (from pyldavis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyldavis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyldavis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyldavis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyldavis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyldavis) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyldavis) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyldavis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyldavis) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyldavis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyldavis) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyldavis) (1.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importation des librairies nécessaires\n",
        "\n",
        "Ce code permet d'importer l'ensemble des bibliothèques nécessaires pour le script. **pandas** est utilisé pour le chargement et la manipulation des données, **re** pour les expressions régulières (pour le prétraitement du texte), **sklearn** pour le topic modeling, **nltk** pour le traitement du langage naturel et **pyLDAvis** pour la visualisation."
      ],
      "metadata": {
        "id": "3hYrThS7Pjoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk"
      ],
      "metadata": {
        "id": "Zry4qhfRRMa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous téléchargeons ensuite les ressources NLTK nécessaires : **wordnet** pour la lemmatisation ainsi que la liste prédéfinie des stopwords."
      ],
      "metadata": {
        "id": "uLiiSSvTRLlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Téléchargement des ressources NLTK pour le prétraitement du texte\n",
        "nltk.download('wordnet') # package pour la limmatisation\n",
        "nltk.download('stopwords') # package pour les stop words\n",
        "nltk.download('punkt') # package pour le tokenisation\n",
        "nltk.download('averaged_perceptron_tagger') # package pour le POS tagging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo6NbHHaR2Px",
        "outputId": "8c8d2d84-7e04-4524-99bc-363bbc4b8250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prétraitement du texte\n",
        "\n",
        "Nous initalisaons le lemmatizer et la liste des stopwords et nous définissons une fonction **preprocess_text** pour le prétraitement des avis. Cette fonction nettoie le texte, le convertit en minuscules, supprime les stopwords, et applique la lemmatisation."
      ],
      "metadata": {
        "id": "hg2QUWzxR3fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les données\n",
        "data = pd.read_csv(\"electronics_sample.csv\")\n",
        "data = data.dropna(subset=['reviewText'])\n",
        "text_data = data['reviewText'].values\n",
        "print(text_data[0 : 5])"
      ],
      "metadata": {
        "id": "n7yeYiJkShvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1bb6ce9-0bf6-4656-e6ae-5b5120905c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tech support is the worst'\n",
            " 'Screws were missing from the bracket and beauty dish within a week.\\nSpend a little more and get much better.'\n",
            " 'Trouble connecting and staying connected via bluetooth'\n",
            " \"I purchased this unit for our RV to replace an older AM/FM/Cassette Tape unit.  I was looking for the CD and Bluetooth capability as well as higher quality sound and this unit certainly delivers all of that.  I found the system for setting up the unit took some getting used to.  I believe this process would be simpler if Alpine had provided 1 or 2 more buttons so there was less button multi-tasking.  It took several attempts to finally get a device recognized and connected on bluetooth.  So far, the receiver has linked to my iPad but refuses to link to my iPod Touch or my cell phone.  It also took several attempts to update the receiver firmware but that was finally achieved.  The update has not solved the bluetooth device linking problem.  I plan to contact Alpine tech support to see if they can resolve this issue.  Overall, this is part of a new product line for Alpine and it feels like I'm an early adopter working on the bleeding edge of a new product design.\"\n",
            " \"It works.  Nuff said but the review requires 16 more words, now 9 more words.  It's blue and has a hood at each end.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Initialisation du lemmatizer et des stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Initilisation du lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "HxtEqhacSQ3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple de part of speech tagging (POS)\n",
        "def tag_parts_of_speech(sentence):\n",
        "    # Tokeniser la phrase en mots\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "\n",
        "    # Obtenir les étiquettes de partie du discours\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "    return pos_tags\n",
        "\n",
        "# Exemple d'utilisation\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "pos_tags = tag_parts_of_speech(sentence)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzQgtlcH6--U",
        "outputId": "aab95613-84ab-462d-c406-aee2c471e430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction de prétraitement qui applique les tâches suivantes :\n",
        "\n",
        "# Convertir le texte en minuscules et remplacer les caractères non alphabétiques par des espaces\n",
        "# Diviser le texte en mots (tokenisation)\n",
        "# Fait du Part of Speech tagging\n",
        "# Supprimer les verbes\n",
        "# Supprimer les stopwords\n",
        "# Appliquer la lemmatisation\n",
        "# Retourner le texte prétraité\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convertir le texte en minuscules et remplacer les caractères non alphabétiques par des espaces\n",
        "\n",
        "    # Diviser le texte en mots (tokenisation)\n",
        "\n",
        "    # Initialiser le lemmatiseur\n",
        "\n",
        "            # Supprimer les verbes\n",
        "\n",
        "                # Lemmatizer le token\n",
        "\n",
        "    return\n",
        "\n",
        "# Application du prétraitement\n",
        "preprocessed_text = [preprocess_text(text) for text in text_data]"
      ],
      "metadata": {
        "id": "i-JI_sFC-cIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement des données\n",
        "\n",
        "Ici, nous chargeons les données à partir d'un fichier CSV, nettoyons les données en supprimant les lignes sans avis, puis appliquons la fonction de prétraitement à chaque avis."
      ],
      "metadata": {
        "id": "28bjQ7BuSTgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorisation et modélisation\n",
        "\n",
        "Avant d'effectuer la modélisation, les mots sont transformés en valeurs numériques (vecteurs) à l'aide de **CountVectorizer**. Celui-ci peut considérer ausi bien des 1-gram que des bigrams, etc. Les mots sont ensuite filtrés à l'aide de **max_df** et **min_df** pour ignorer les mots qui sont trop fréquents ou trop rares. Cela permet à l'algorithme LDA de se concentrer sur les mots significatifs et réduit la dimensionnalité du dataset. Le modèle est entraîné sur la matrice document-terme formée.\n"
      ],
      "metadata": {
        "id": "16uAVPuoS4AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Vectorisation du texte avec bigrammes\n",
        "\n",
        "# Création et ajustement du modèle LDA\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=0, n_jobs=1)\n",
        "lda.fit(dtm)"
      ],
      "metadata": {
        "id": "Rc1akIYgVM0x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "bb1cbaf7-404c-47b8-ef51-c0ac965c630d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(n_components=5, n_jobs=1, random_state=0)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=5, n_jobs=1, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=5, n_jobs=1, random_state=0)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Visualisation des thématiques\n",
        "\n",
        " Ce code prépare les données pour **pyLDAvis** et renvoie une visualisation interactive des sujets trouvés dans les avis. Cela permet une exploration visuelle et intuitive des thématiques et de leurs mots clés associés."
      ],
      "metadata": {
        "id": "aMYb5GgxVabC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model\n",
        "# Préparation des données pour pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "lda_display = pyLDAvis.lda_model.prepare(lda, dtm, vectorizer, sort_topics=False)\n",
        "\n",
        "# Affichage de la visualisation\n",
        "pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "id": "oUfinABKXAQ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}