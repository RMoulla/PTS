{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvczFXXZj4Em2qtbib+5bh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/PTS/blob/main/TF_IDF_Correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Travaux pratiques : Développement d'un moteur de recherche avec TF-IDF\n",
        "Dans ce TP, nous allons développer un moteur de recherche basé sur la technique TF-IDF (Term Frequency-Inverse Document Frequency) appliquée aux critiques de films issues de la base de données IMDB. L'objectif principal de ce TP est d'apprendre à prétraiter des données textuelles, à transformer des textes en représentations numériques exploitables et à construire un système de recherche permettant de retrouver des documents pertinents en fonction de requêtes utilisateur.\n",
        "\n",
        "## Objectifs\n",
        "* Prétraitement des Données Textuelles : Apprendre à nettoyer et préparer les données textuelles en éliminant les balises HTML, en convertissant les textes en minuscules, et en appliquant la lemmatisation pour normaliser les mots.\n",
        "* Transformation en Matrice TF-IDF : Utiliser le vectoriseur TF-IDF pour convertir les critiques de films en une matrice numérique où chaque dimension représente l'importance d'un terme dans un document.\n",
        "*Développement d'un moteur de recherche : Implémenter un moteur de recherche simple qui utilise la similarité cosinus pour retrouver les critiques les plus pertinentes par rapport à une requête utilisateur.\n",
        "\n",
        "## Outils Utilisés\n",
        "* Pandas : Pour le chargement et la manipulation des données.\n",
        "* NLTK (Natural Language Toolkit) : Pour les opérations de prétraitement telles que la lemmatisation et l'élimination des stop words.\n",
        "* Scikit-learn : Pour la transformation des textes en matrices TF-IDF et l'implémentation des algorithmes de recherche.\n",
        "\n",
        "## Étapes du TP\n",
        "* Chargement des données : Importer les critiques de films à partir d'un fichier CSV et afficher les premières lignes pour comprendre la structure des données.\n",
        "* Nettoyage et prétraitement : Appliquer des techniques de nettoyage des textes, comme la suppression des balises HTML et la conversion en minuscules. Utiliser NLTK pour lemmatiser les mots, ce qui permet de réduire les variations grammaticales.\n",
        "* Transformation TF-IDF : Initialiser un TfidfVectorizer pour convertir les critiques en une matrice TF-IDF. Cette matrice permettra de quantifier l'importance des termes dans chaque critique.\n",
        "* Développement du moteur de recherche : Implémenter une fonction de recherche qui utilise la similarité cosinus pour mesurer la pertinence des critiques par rapport à une requête donnée. Afficher les résultats de recherche de manière lisible.\n",
        "* Évaluation et discussion : Tester le moteur de recherche avec différentes requêtes, analyser les résultats obtenus, et discuter des améliorations possibles pour optimiser les performances du système."
      ],
      "metadata": {
        "id": "_MwHD4bIq-ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement des données"
      ],
      "metadata": {
        "id": "o9AweWwusBZU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVYDQ9ykhEWC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger le dataset\n",
        "df = pd.read_csv('IMDB.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimer la première ligne du dataframe\n",
        "print(df['review'].loc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOQyY4YEjfW9",
        "outputId": "1958415a-155e-4814-884b-8cff6b62aa2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prétraitement des données\n",
        "\n",
        "Cette section du code se concentre sur le prétraitement des critiques de films afin de les préparer pour une analyse ultérieure. Le prétraitement est une étape cruciale dans le traitement du langage naturel (NLP) car il permet de nettoyer et de normaliser les données textuelles.\n",
        "\n",
        "1. **Importation des Bibliothèques NLTK :**\n",
        "\n",
        "* `nltk` est importé avec des modules spécifiques tels que stopwords et wordnet pour le traitement linguistique.\n",
        "* `WordNetLemmatizer` est importé pour effectuer la lemmatisation, une technique qui réduit les mots à leur forme de base.\n",
        "\n",
        "2. **Nettoyage des Données :**\n",
        "\n",
        "Les critiques de films, stockées dans la colonne review du dataframe df, sont nettoyées en supprimant les balises HTML (`<br />`) et en convertissant tout le texte en minuscules. Cela permet d'uniformiser les données et de faciliter l'analyse.\n",
        "\n",
        "3. **Définition de la Fonction de Lemmatisation :**\n",
        "\n",
        "La fonction `lemmatize_text` prend un texte en entrée, le divise en mots (`split`), et lemmatise chaque mot en utilisant `WordNetLemmatizer`, en considérant chaque mot comme un verbe (`pos=wordnet.VERB`), et en excluant les stop words.\n",
        "\n",
        "4. **Application de la Lemmatisation :**\n",
        "\n",
        "La fonction `lemmatize_text` est appliquée à chaque critique de film dans la colonne `review` du dataframe `df`."
      ],
      "metadata": {
        "id": "xA6r5Dt-juOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Nettoyer les données : suppression des balises HTML et conversion en minuscules\n",
        "df['review'] = df['review'].str.replace('<br />', ' ')  # Suppression des balises HTML\n",
        "df['review'] = df['review'].str.lower()  # Conversion en minuscules\n",
        "\n",
        "# Initialiser le lemmatiseur WordNet\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))  # Chargement des stop words\n",
        "\n",
        "# Fonction de lemmatisation\n",
        "def lemmatize_text(text):\n",
        "    words = text.split()  # Extraction des mots\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words if word not in stop_words]  # Lemmatisation\n",
        "    return ' '.join(lemmatized_words)  # Reconstruction du texte\n",
        "\n",
        "# Appliquer la lemmatisation\n",
        "df['review'] = df['review'].apply(lemmatize_text)\n",
        "\n",
        "# Afficher la première ligne du dataset prétraité\n",
        "print(df['review'].loc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFJ_xvwSjztO",
        "outputId": "f0be84b8-4141-4e77-a69f-551524f9fb52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one reviewers mention watch 1 oz episode hooked. right, exactly happen me. first thing strike oz brutality unflinching scenes violence, set right word go. trust me, show faint hearted timid. show pull punch regard drugs, sex violence. hardcore, classic use word. call oz nickname give oswald maximum security state penitentary. focus mainly emerald city, experimental section prison cells glass front face inwards, privacy high agenda. em city home many..aryans, muslims, gangstas, latinos, christians, italians, irish more....so scuffles, death stares, dodgy deal shady agreements never far away. would say main appeal show due fact go show dare. forget pretty picture paint mainstream audiences, forget charm, forget romance...oz mess around. first episode ever saw strike nasty surreal, say ready it, watch more, develop taste oz, get accustom high level graphic violence. violence, injustice (crooked guard who'll sell nickel, inmates who'll kill order get away it, well mannered, middle class inmates turn prison bitch due lack street skills prison experience) watch oz, may become comfortable uncomfortable viewing....thats get touch darker side.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformation en matrice TF-IDF\n",
        "\n",
        "Cette section du code concerne la transformation des critiques de films prétraitées en une matrice TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "\n",
        "1. **Importation du Module TfidfVectorizer de scikit-learn :**\n",
        "\n",
        "Le `TfidfVectorizer` de la bibliothèque `scikit-learn` est utilisé pour convertir les textes en une matrice TF-IDF. Cette méthode permet de quantifier l'importance des termes dans les documents en tenant compte de leur fréquence dans les documents et de leur rareté dans l'ensemble des documents.\n",
        "\n",
        "2. **Initialisation du Vectorizer :**\n",
        "\n",
        "Un objet `TfidfVectorizer` est initialisé avec les paramètres suivants :\n",
        "* `max_features=1000` : Limite le nombre de termes à 1000, en se concentrant sur les termes les plus fréquents.\n",
        "* `stop_words='english'` : Exclut les stop words anglais, qui sont des mots courants n'apportant pas beaucoup de sens (comme \"the\", \"is\", etc.).\n",
        "\n",
        "3. **Transformation des données :**\n",
        "\n",
        "Le `fit_transform` est appliqué sur les critiques de films prétraitées stockées dans `df['review']`. Cette méthode effectue typiquement deux opérations :\n",
        "* `fit` : Apprend le vocabulaire du texte et calcule les statistiques TF-IDF pour chaque terme.\n",
        "* `transform` : Convertit les critiques en vecteurs TF-IDF en utilisant les statistiques apprises."
      ],
      "metadata": {
        "id": "DvRNpWUYu3yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialiser le vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "\n",
        "# Ajuster sur les données et transformer les données\n",
        "X = vectorizer.fit_transform(df['review'])\n",
        "\n",
        "print(\"Shape of TF-IDF matrix:\", X.shape)\n"
      ],
      "metadata": {
        "id": "grgvV1P6n843"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0].toarray()"
      ],
      "metadata": {
        "id": "fULF6oZdoC8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implémentation du moteur de Recherche\n",
        "Cette section du code implémente un moteur de recherche qui permet de retrouver les critiques de films les plus pertinentes en fonction d'une requête utilisateur. Le moteur de recherche utilise la similarité cosinus pour mesurer la pertinence des critiques par rapport à la requête.\n",
        "\n",
        "1. **Importation des bibliothèques nécessaires :**\n",
        "\n",
        "* `numpy` pour la manipulation des tableaux et le tri des indices.\n",
        "* `cosine_similarity` de `sklearn.metrics.pairwise` pour calculer la similarité entre les vecteurs TF-IDF.\n",
        "\n",
        "2. **Définition de la fonction search :**\n",
        "\n",
        "La fonction `search` prend une requête utilisateur, le vectoriseur TF-IDF, la matrice TF-IDF des critiques de films, le dataframe contenant les critiques, et le nombre de résultats à retourner (`top_n`).\n",
        "\n",
        "3. **Transformation de la Requête en Vecteur TF-IDF :**\n",
        "\n",
        "La requête utilisateur est transformée en vecteur TF-IDF en utilisant le vectoriseur initialisé précédemment.\n",
        "\n",
        "4. **Calcul de la similarité cosinus :**\n",
        "\n",
        "La similarité cosinus est calculée entre le vecteur TF-IDF de la requête et la matrice TF-IDF des critiques. La similarité cosinus mesure la proximité entre deux vecteurs dans un espace multidimensionnel.\n",
        "\n",
        "5. **Tri des critiques par similarité :**\n",
        "\n",
        "* Les indices des critiques sont triés en ordre décroissant de similarité.\n",
        "* Les indices des `top_n` critiques les plus similaires sont sélectionnés.\n",
        "* La fonction retourne les indices des critiques les plus similaires et leurs scores de similarité.\n"
      ],
      "metadata": {
        "id": "BgWXT94owLxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def search(query, vectorizer, tfidf_matrix, df, top_n=5):\n",
        "    # Transformer la requête en vecteur TF-IDF\n",
        "    query_vec = vectorizer.transform([query])\n",
        "\n",
        "    # Calculer la similarité cosinus entre la requête et les critiques\n",
        "    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "\n",
        "    # Trouver les indices des critiques les plus similaires\n",
        "    indices = np.argsort(similarities)[::-1][:top_n]\n",
        "\n",
        "    # Retourner les critiques les plus similaires\n",
        "    return indices, similarities[indices]\n",
        "\n",
        "# Exemple d'utilisation\n",
        "query = \"great movie with excellent acting\"\n",
        "indices, similarities = search(query, vectorizer, X, df)\n",
        "\n",
        "# Afficher les résultats\n",
        "for idx, similarity in zip(indices, similarities):\n",
        "    print(f\"Similarity: {similarity:.4f}\")\n",
        "    print(f\"Review: {df.iloc[idx]['review']}\\n\")\n",
        "    print(f\"Sentiment: {df.iloc[idx]['sentiment']}\\n\")\n",
        "    print(\"=\"*50 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "JuauqLnKo6yE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}